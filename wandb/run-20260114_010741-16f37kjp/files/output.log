2026-01-14 01:07:42 - INFO - __main__ - Wandb initialized: https://wandb.ai/alfredcs_team/orthgsa-qwen3-4b/runs/16f37kjp
2026-01-14 01:07:43 - INFO - __main__ - Loading model: Qwen/Qwen3-4B-Instruct-2507
2026-01-14 01:07:43 - INFO - orthgsa.models.orthgsa_model - Loading base model: Qwen/Qwen3-4B-Instruct-2507
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66.36it/s]
2026-01-14 01:07:43 - INFO - orthgsa.models.orthgsa_model - OrthGSA model initialized with 4 streams
2026-01-14 01:07:43 - INFO - __main__ - Model parameters: 4031.69M total, 4031.69M trainable
2026-01-14 01:07:43 - INFO - __main__ - Gradient checkpointing enabled
/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py:2372: UserWarning: You passed find_unused_parameters=true to DistributedDataParallel, `_set_static_graph` will detect unused parameters automatically, so you do not need to set find_unused_parameters=true, just be sure these unused parameters will not change during training loop while calling `_set_static_graph`.
  warnings.warn(
2026-01-14 01:07:45 - INFO - __main__ - Model wrapped with DDP
2026-01-14 01:07:45 - INFO - orthgsa.utils.training_utils - Optimizer groups: 397 decay params, 363 no-decay params
2026-01-14 01:07:45 - INFO - orthgsa.utils.training_utils - Using fused AdamW optimizer
2026-01-14 01:07:45 - INFO - orthgsa.utils.training_utils - LR scheduler: cosine, warmup=3000, total=100000
2026-01-14 01:07:45 - INFO - orthgsa.data.slimpajama - Loading from local path: /home/alfred/datasets/SlimPajama-627B (streaming, packed, split=train)
2026-01-14 01:09:44 - INFO - orthgsa.data.slimpajama - Loading from local path: /home/alfred/datasets/SlimPajama-627B (streaming, packed, split=train)
2026-01-14 01:11:42 - INFO - __main__ - Starting training...
Training:   0%|                                                                                                                                             | 1/100000 [01:24<2335:34:09, 84.08s/it]Traceback (most recent call last):
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 459, in <module>
    main()
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 361, in main
    step_metrics = train_step(
                   ^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 113, in train_step
    outputs = model(
              ^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1661, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1487, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
