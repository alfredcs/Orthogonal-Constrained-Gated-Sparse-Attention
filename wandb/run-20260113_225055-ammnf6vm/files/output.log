2026-01-13 22:50:56 - INFO - __main__ - Wandb initialized: https://wandb.ai/alfredcs_team/orthgsa-qwen3-4b/runs/ammnf6vm
2026-01-13 22:50:56 - INFO - __main__ - Loading model: Qwen/Qwen3-4B-Instruct-2507
2026-01-13 22:50:56 - INFO - orthgsa.models.orthgsa_model - Loading base model: Qwen/Qwen3-4B-Instruct-2507
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 69.22it/s]
2026-01-13 22:50:57 - INFO - orthgsa.models.orthgsa_model - OrthGSA model initialized with 4 streams
2026-01-13 22:50:57 - INFO - __main__ - Model parameters: 4031.69M total, 4031.69M trainable
2026-01-13 22:50:57 - INFO - __main__ - Gradient checkpointing enabled
Traceback (most recent call last):
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 451, in <module>
    main()
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 252, in main
    model = wrap_model_fsdp(model, **fsdp_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/orthgsa/utils/training_utils.py", line 173, in wrap_model_fsdp
    return FSDP(model, **fsdp_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 494, in __init__
    _init_param_handle_from_module(
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py", line 622, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py", line 634, in _init_param_handle_from_params
    handle = FlatParamHandle(
             ^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_flat_param.py", line 588, in __init__
    self._init_flat_param_and_metadata(
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_flat_param.py", line 644, in _init_flat_param_and_metadata
    ) = self._validate_tensors_to_flatten(params)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_flat_param.py", line 788, in _validate_tensors_to_flatten
    raise ValueError(
ValueError: Must flatten tensors with uniform dtype but got torch.float32 and torch.bfloat16
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 451, in <module>
[rank0]:     main()
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/scripts/train.py", line 252, in main
[rank0]:     model = wrap_model_fsdp(model, **fsdp_config)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/orthgsa/utils/training_utils.py", line 173, in wrap_model_fsdp
[rank0]:     return FSDP(model, **fsdp_kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 494, in __init__
[rank0]:     _init_param_handle_from_module(
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py", line 622, in _init_param_handle_from_module
[rank0]:     _init_param_handle_from_params(state, managed_params, fully_sharded_module)
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_init_utils.py", line 634, in _init_param_handle_from_params
[rank0]:     handle = FlatParamHandle(
[rank0]:              ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_flat_param.py", line 588, in __init__
[rank0]:     self._init_flat_param_and_metadata(
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_flat_param.py", line 644, in _init_flat_param_and_metadata
[rank0]:     ) = self._validate_tensors_to_flatten(params)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/alfred/codes/Orthogonal-Constrained-Gated-Sparse-Attention/.venv/lib/python3.11/site-packages/torch/distributed/fsdp/_flat_param.py", line 788, in _validate_tensors_to_flatten
[rank0]:     raise ValueError(
[rank0]: ValueError: Must flatten tensors with uniform dtype but got torch.float32 and torch.bfloat16
