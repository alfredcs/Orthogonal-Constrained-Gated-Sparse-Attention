# OrthGSA Training Configuration for Qwen3-8B on SlimPajama-627B
# Target: 8x 44GB GPUs, 64K context window
# Memory estimate: ~40-44GB per GPU with DeepSpeed ZeRO-3 + CPU offload

# Model Configuration
model:
  base_model: "/home/alfred/models/Qwen3-8B"

  # OrthGSA-specific parameters (using Cayley Transform for orthogonal constraints)
  orthgsa:
    n_streams: 2                    # Minimum streams for 64K context memory
    alpha_init: 0.01                # Initial scaling for orthogonal HC coefficients
    cayley_scaling: 0.1             # Scaling for Cayley transform input

  # GSA parameters - tuned for 64K context with tight memory
  gsa:
    enabled: true
    k_base: 384                     # Reduced for memory
    k_min: 96                       # Minimum k
    k_max: 768                      # Maximum k for 64K context
    indexer_heads: 4                # Reduced indexer heads
    indexer_dim: 64                 # Reduced indexer dimension
    adaptive_k: true                # Enable adaptive k selection
    gate_bias_init: 0.5             # Initial gate bias

# Data Configuration
# Supports S3 streaming, HuggingFace hub, and local datasets
data:
  dataset: "cerebras/SlimPajama-627B"  # Fallback HuggingFace identifier
  dataset_path: "s3://public-datasets-multimodality/SlimPajama-627B/"  # S3 path (streams directly, no download)
  max_seq_length: 65536             # 64K context window
  num_workers: 2                    # Reduced workers to save memory
  streaming: true                   # Use streaming for large dataset

# Training Configuration
training:
  # Batch sizes optimized for 8x44GB GPUs with 64K context
  per_device_train_batch_size: 1    # Per GPU batch size (must be 1 for 64K)
  per_device_eval_batch_size: 1     # Same for eval
  gradient_accumulation_steps: 8    # Reduced for memory, effective batch = 8*1*8 = 64

  # Learning rate (further reduced for longer context training)
  learning_rate: 1.2e-5             # Peak learning rate
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05                # Longer warmup for stability
  weight_decay: 0.1
  max_grad_norm: 0.5                # Tighter gradient clipping

  # Training duration
  max_steps: 2000                   # Total training steps
  eval_steps: 250                   # Evaluate every N steps
  save_steps: 500                   # Save checkpoint every N steps
  logging_steps: 10                 # Log every N steps

  # Precision and optimization
  bf16: true                        # Use BF16 mixed precision
  tf32: true                        # Enable TF32 on Ampere+
  gradient_checkpointing: true      # Critical for long context memory

  # Optimizer
  optim: "adamw_torch_fused"        # Fused AdamW
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8

# Distributed Training Configuration
# IMPORTANT: 8B model + 64K context requires ZeRO-3 with CPU offloading on 44GB GPUs
distributed:
  strategy: "deepspeed"             # DeepSpeed for multi-GPU
  deepspeed_config: "configs/deepspeed_zero3_64k.json"

  # FSDP config (alternative)
  fsdp_config:
    sharding_strategy: "FULL_SHARD"
    cpu_offload: true               # Enable CPU offload
    backward_prefetch: "BACKWARD_PRE"
    state_dict_type: "FULL_STATE_DICT"
    limit_all_gathers: true
    forward_prefetch: true
    sync_module_states: true
    use_orig_params: true

# Memory Optimization
memory:
  # Target ~95% of 44GB = ~42GB per GPU (very tight)
  target_memory_utilization: 0.95

  # Activation checkpointing - essential for 64K context
  activation_checkpointing: true
  checkpoint_layers: "all"

  # Efficient attention - critical for long context
  use_flash_attention: true

  # Batch finding
  auto_find_batch_size: false

# Logging Configuration
logging:
  wandb:
    enabled: true
    project: "orthgsa-qwen3-8b"
    entity: null
    run_name: "qwen3-8b-64k-ctx"
    tags: ["orthgsa", "qwen3-8b", "slimpajama", "64k-context", "cayley-transform", "zero3"]
    log_model: false

  output_dir: "./outputs/orthgsa-qwen3-8b-64k"
  logging_dir: "./logs"
  report_to: ["wandb", "tensorboard"]

# Checkpointing
checkpointing:
  save_total_limit: 2               # Fewer checkpoints due to size
  save_on_each_node: false
  resume_from_checkpoint: null

# Evaluation
evaluation:
  do_eval: true
  eval_strategy: "steps"
  per_device_eval_batch_size: 1
  eval_accumulation_steps: 4

# Seed for reproducibility
seed: 42
