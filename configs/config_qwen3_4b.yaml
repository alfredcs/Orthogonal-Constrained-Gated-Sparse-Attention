# OrthGSA Training Configuration for Qwen3-4B on SlimPajama-627B
# Target: 4x 44GB GPUs, >65% memory usage, >90% utilization

# Model Configuration
model:
  base_model: "Qwen/Qwen3-4B-Instruct-2507"

  # OrthGSA-specific parameters (using Cayley Transform for orthogonal constraints)
  orthgsa:
    n_streams: 2                    # Number of residual streams (reduced from 4 for faster training)
    alpha_init: 0.01                # Initial scaling for orthogonal HC coefficients
    cayley_scaling: 0.1             # Scaling for Cayley transform input

  # GSA parameters
  gsa:
    enabled: true
    k_base: 512                     # Base number of selected tokens
    k_min: 128                      # Minimum k
    k_max: 1024                     # Maximum k
    indexer_heads: 4                # Number of indexer heads
    indexer_dim: 64                 # Indexer dimension
    adaptive_k: true                # Enable adaptive k selection
    gate_bias_init: 0.5             # Initial gate bias

# Data Configuration
# Supports both streaming from HuggingFace and pre-downloaded local datasets
# Local path is used automatically if the directory exists and is not empty
# To download locally: python -c "from datasets import load_dataset; load_dataset('cerebras/SlimPajama-627B').save_to_disk('~/datasets/SlimPajama-627B')"
data:
  dataset: "cerebras/SlimPajama-627B"
  local_path: "~/datasets/SlimPajama-627B"  # Pre-downloaded dataset path (recommended to avoid rate limits)
  max_seq_length: 1024              # Reduced sequence length for memory (increase for longer context)
  num_workers: 2                    # DataLoader workers per GPU (reduced for local loading)
  streaming: true                   # Use streaming for large dataset

# Training Configuration
training:
  # Batch sizes reduced for 4x44GB GPUs with n_streams=4
  per_device_train_batch_size: 1    # Per GPU batch size (reduced for memory)
  per_device_eval_batch_size: 2     # Larger for eval (no gradients)
  gradient_accumulation_steps: 32   # Effective batch = 4*1*32 = 128

  # Learning rate (scaled for effective batch size)
  learning_rate: 2.0e-5             # Peak learning rate
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03                # 3% warmup
  weight_decay: 0.1
  max_grad_norm: 1.0

  # Training duration
  max_steps: 100000                 # Total training steps
  eval_steps: 1000                  # Evaluate every N steps
  save_steps: 2000                  # Save checkpoint every N steps
  logging_steps: 10                 # Log every N steps

  # Precision and optimization
  bf16: true                        # Use BF16 mixed precision
  tf32: true                        # Enable TF32 on Ampere+
  gradient_checkpointing: true      # Memory optimization

  # Optimizer
  optim: "adamw_torch_fused"        # Fused AdamW
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8

# Distributed Training Configuration
# For multi-GPU training, we recommend DeepSpeed ZeRO-2 over DDP/FSDP
# DeepSpeed ZeRO-2 reduces memory from ~44GB to ~17GB per GPU (for 4B model)
#
# Training methods comparison:
#   - DDP (torchrun + train.py): ~44GB/GPU - requires 80GB+ GPUs
#   - DeepSpeed ZeRO-2 (deepspeed + train_deepspeed.py): ~17GB/GPU - recommended
#   - DeepSpeed ZeRO-3: ~12GB/GPU - for very memory-constrained setups
#
# Launch commands:
#   DeepSpeed: deepspeed --num_gpus=4 scripts/train_deepspeed.py --config configs/config_qwen3_4b.yaml
#   DDP:       torchrun --nproc_per_node=4 scripts/train.py --config configs/config_qwen3_4b.yaml
distributed:
  strategy: "deepspeed"             # Recommended: use DeepSpeed for multi-GPU
  # Alternative: "fsdp" or "ddp" (requires more memory per GPU)

  # DeepSpeed config is embedded in train_deepspeed.py
  # Key settings: ZeRO stage 2, optimizer sharding, BF16
  deepspeed_config: null            # Uses embedded config in train_deepspeed.py

  # FSDP config (alternative, used with train.py)
  fsdp_config:
    sharding_strategy: "FULL_SHARD" # Full sharding
    cpu_offload: false              # Keep on GPU for speed
    backward_prefetch: "BACKWARD_PRE"
    state_dict_type: "FULL_STATE_DICT"
    limit_all_gathers: true
    forward_prefetch: true
    sync_module_states: true
    use_orig_params: true

# Memory Optimization
memory:
  # Target >65% of 44GB = ~28.6GB per GPU
  target_memory_utilization: 0.68

  # Activation checkpointing
  activation_checkpointing: true
  checkpoint_layers: "all"          # Checkpoint all layers

  # Efficient attention
  use_flash_attention: true

  # Batch finding
  auto_find_batch_size: false       # We've pre-calculated

# Logging Configuration
logging:
  wandb:
    enabled: true
    project: "orthgsa-qwen3-4b"
    entity: null                    # Set your wandb entity
    run_name: null                  # Auto-generated if null
    tags: ["orthgsa", "qwen3-4b", "slimpajama", "cayley-transform"]
    log_model: false                # Don't upload model to wandb

  # Local logging
  output_dir: "./outputs/orthgsa-qwen3-4b"
  logging_dir: "./logs"
  report_to: ["wandb", "tensorboard"]

# Checkpointing
checkpointing:
  save_total_limit: 3               # Keep last 3 checkpoints
  save_on_each_node: false
  resume_from_checkpoint: null      # Path to resume from

# Evaluation
evaluation:
  do_eval: true
  eval_strategy: "steps"
  per_device_eval_batch_size: 2
  eval_accumulation_steps: 4

# Seed for reproducibility
seed: 42
