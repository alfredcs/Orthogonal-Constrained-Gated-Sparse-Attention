# OrthGSA Training Configuration for Qwen3-4B on SlimPajama-627B
# Target: 4x 44GB GPUs, >65% memory usage, >90% utilization

# Model Configuration
model:
  base_model: "Qwen/Qwen3-4B-Instruct-2507"

  # OrthGSA-specific parameters (using Cayley Transform for orthogonal constraints)
  orthgsa:
    n_streams: 4                    # Number of residual streams
    alpha_init: 0.01                # Initial scaling for orthogonal HC coefficients
    cayley_scaling: 0.1             # Scaling for Cayley transform input

  # GSA parameters
  gsa:
    enabled: true
    k_base: 512                     # Base number of selected tokens
    k_min: 128                      # Minimum k
    k_max: 1024                     # Maximum k
    indexer_heads: 4                # Number of indexer heads
    indexer_dim: 64                 # Indexer dimension
    adaptive_k: true                # Enable adaptive k selection
    gate_bias_init: 0.5             # Initial gate bias

# Data Configuration
data:
  dataset: "cerebras/SlimPajama-627B"
  max_seq_length: 2048              # Sequence length
  num_workers: 4                    # DataLoader workers per GPU
  streaming: true                   # Use streaming for large dataset

# Training Configuration
training:
  # Batch sizes optimized for 4x44GB GPUs
  per_device_train_batch_size: 4    # Per GPU batch size
  per_device_eval_batch_size: 8     # Larger for eval (no gradients)
  gradient_accumulation_steps: 8    # Effective batch = 4*4*8 = 128

  # Learning rate (scaled for effective batch size)
  learning_rate: 2.0e-5             # Peak learning rate
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03                # 3% warmup
  weight_decay: 0.1
  max_grad_norm: 1.0

  # Training duration
  max_steps: 100000                 # Total training steps
  eval_steps: 1000                  # Evaluate every N steps
  save_steps: 2000                  # Save checkpoint every N steps
  logging_steps: 10                 # Log every N steps

  # Precision and optimization
  bf16: true                        # Use BF16 mixed precision
  tf32: true                        # Enable TF32 on Ampere+
  gradient_checkpointing: true      # Memory optimization

  # Optimizer
  optim: "adamw_torch_fused"        # Fused AdamW
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8

# Distributed Training Configuration
distributed:
  strategy: "fsdp"                  # Use FSDP for 4 GPUs
  fsdp_config:
    sharding_strategy: "FULL_SHARD" # Full sharding
    cpu_offload: false              # Keep on GPU for speed
    backward_prefetch: "BACKWARD_PRE"
    state_dict_type: "FULL_STATE_DICT"
    limit_all_gathers: true
    forward_prefetch: true
    sync_module_states: true
    use_orig_params: true

  # DeepSpeed alternative (if needed)
  deepspeed_config: null

# Memory Optimization
memory:
  # Target >65% of 44GB = ~28.6GB per GPU
  target_memory_utilization: 0.68

  # Activation checkpointing
  activation_checkpointing: true
  checkpoint_layers: "all"          # Checkpoint all layers

  # Efficient attention
  use_flash_attention: true

  # Batch finding
  auto_find_batch_size: false       # We've pre-calculated

# Logging Configuration
logging:
  wandb:
    enabled: true
    project: "orthgsa-qwen3-4b"
    entity: null                    # Set your wandb entity
    run_name: null                  # Auto-generated if null
    tags: ["orthgsa", "qwen3-4b", "slimpajama", "cayley-transform"]
    log_model: false                # Don't upload model to wandb

  # Local logging
  output_dir: "./outputs/orthgsa-qwen3-4b"
  logging_dir: "./logs"
  report_to: ["wandb", "tensorboard"]

# Checkpointing
checkpointing:
  save_total_limit: 3               # Keep last 3 checkpoints
  save_on_each_node: false
  resume_from_checkpoint: null      # Path to resume from

# Evaluation
evaluation:
  do_eval: true
  eval_strategy: "steps"
  per_device_eval_batch_size: 8
  eval_accumulation_steps: 4

# Seed for reproducibility
seed: 42
